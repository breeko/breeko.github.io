<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns#">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <title>Offline Object Detection and Tracking on a Raspberry Pi &middot; Branko Blagojevic</title>
        <meta name="description" content="Load and run YOLO (You Only Look Once) object detection model on a Raspberry Pi and track objects throughout the day.
In my last post I wrote about the YOLO model used for object detection. The most surprising thing was how simple the model is. It’s so simple that it can run offline on a raspberry pi
In my last post I wrote about the YOLO (You Only Look Once) model used for object detection.">
        <meta name="HandheldFriendly" content="True">
        <meta name="MobileOptimized" content="320">
        <meta name="generator" content="Hugo 0.54.0" />
        <meta name="robots" content="index,follow">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta property="og:title" content="Offline Object Detection and Tracking on a Raspberry Pi">
<meta property="og:description" content="Load and run YOLO (You Only Look Once) object detection model on a Raspberry Pi and track objects throughout the day.
In my last post I wrote about the YOLO model used for object detection. The most surprising thing was how simple the model is. It’s so simple that it can run offline on a raspberry pi
In my last post I wrote about the YOLO (You Only Look Once) model used for object detection.">
<meta property="og:type" content="article">
<meta property="og:url" content="http://example.org/post/2018-05-09_offline-object-detection-and-tracking-on-a-raspberry-pi/">
        <link rel="stylesheet" href="http://example.org/dist/styles.css">
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,400,600,700,300&subset=latin,cyrillic-ext,latin-ext,cyrillic">
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
        
        
        
    </head>
    <body>
        

        <div id="wrapper">
            <header class="site-header">
                <div class="container">
                    <div class="site-title-wrapper">
                        
                            <h1 class="site-title">
                                <a title="ML-Everything" href="http://example.org/">ML-Everything</a>
                            </h1>
                        
                        <a class="button-square" href="http://example.org/index.xml"><i class="fa fa-rss"></i></a>
                        
                        
                        
                            <a class="button-square button-social hint--top" data-hint="Github" title="Github" href="https://github.com/breeko">
                                <i class="fa fa-github-alt"></i>
                            </a>
                        
                        
                        
                        
                        
                            <a class="button-square button-social hint--top" data-hint="Email" title="Email" href="mailto:branko.blagojevic@gmail.com">
                                <i class="fa fa-envelope"></i>
                            </a>
                        
                    </div>

                    <ul class="site-nav">
                        
    <li class="site-nav-item">
        <a title="Blog" href="/">Blog</a>
    </li>

                    </ul>
                </div>
            </header>

            <div id="container">


<div class="container">
    <article class="post-container" itemscope="" itemtype="http://schema.org/BlogPosting">
        <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Offline Object Detection and Tracking on a Raspberry Pi</h1>
    
    <p class="post-date post-line">
        <span>Published <time datetime="2018-05-09" itemprop="datePublished">Wed, May 9, 2018</time></span>
        <span>by</span>
        <span itemscope="" itemprop="author" itemtype="https://schema.org/Person">
            <span itemprop="name">
                <a href="#" itemprop="url" rel="author">Branko Blagojevic</a>
            </span>
        </span>
    </p>
    
</header>

        <div class="post-content clearfix" itemprop="articleBody">
    

    

<p>Load and run YOLO (You Only Look Once) object detection model on a Raspberry Pi and track objects throughout the day.</p>

<p>In my last post I wrote about the YOLO model used for object detection. The most surprising thing was how simple the model is. It’s so simple that it can run offline on a raspberry pi</p>

<p>In my <a href="http://example.org/post/2018-04-05_how-to-actually-easily-detect-objects-with-deep-learning-on-raspberry-pi/">last post</a> I wrote about the YOLO (You Only Look Once) model used for object detection. The most surprising thing when reading the paper was how clever and relatively simple the model is. The strength lies in the fact that the model only passes through the image once and from that single pass, is able to detect a number of objects. The architecture also allows mini versions with fewer layers, a smaller size but similar performance.</p>

<p>In fact, the model is so small it can run offline on a weak device such as a rapberry pi. This is just what I did.</p>

<h3 id="tldr">TLDR</h3>

<p>I have two cats and I wanted to track their whereabouts when I was not at home. So I setup a raspberry pi to take periodically take a picture, detect the picture for objects and save the object locations down to a json file. After the objects were processed, I then wrote a script that applies a kind of heatmap on a base image with where the provided objects were over all the logs.</p>

<table>
<thead>
<tr>
<th align="center"><img src="/post/resources/img/1*azur79ZVQn3ISAVO_Q5SRw.png" alt="" /></th>
</tr>
</thead>

<tbody>
<tr>
<td align="center">My cats eagerly awaiting the return of my wife</td>
</tr>
</tbody>
</table>

<p>The project requires tensorflow, keras, numpy and matplotlib to be installed on your raspberry pi. I have some notes below on getting those installed. Below will set everything up and take a photo every minute until stopped, detect object and save them in json format in the logs directory.</p>

<pre><code class="language-sh">git clone [https://github.com/breeko/spypy.git](https://github.com/breeko/spypy.git)  
cd spypy  
./download_weights.sh # or just download weights manually  
python track.py
</code></pre>

<p>Once enough logs are saved, the below will convert the logs from the log directory and overlay the objects onto the input image. The object value has to be an object that the model was trained on, which is visible from the .classes file in the model directory.</p>

<p>python convert.py -i [input_image to draw over] -d [directory of logs] -o [object to detect]</p>

<table>
<thead>
<tr>
<th align="center"><img src="/post/resources/img/1*pfGIMsqfdbb1F1D2rRnj_A.jpeg" alt="" /></th>
</tr>
</thead>

<tbody>
<tr>
<td align="center">I spy a Rapberry Pi</td>
</tr>
</tbody>
</table>

<h4 id="dependencies">Dependencies</h4>

<p>The most frustrating part was getting tensorflow and Keras to run on raspberry pi. Not because the pi is not powerful enough, but the standard methods to install tensorflow did not work for me (e.g. pip) and there is less support for weaker ARM hardware. I was able to install the tensorflow for python 2.7, but 3.4+ was not working for me. I won’t discuss it here in length, but here are the links that worked for me.</p>

<blockquote>
<p><a href="https://nikhilraghava.wordpress.com/2017/08/05/installing-keras-on-raspberry-pi-3/">Install Keras</a></p>

<p><a href="http://ci.tensorflow.org/view/Nightly/job/nightly-pi-zero/lastSuccessfulBuild/artifact//output-artifacts/">Tensorflow wheel</a> (install with pip)</p>

<p><a href="https://github.com/samjabrahams/tensorflow-on-raspberry-pi">Install tensorflow</a></p>
</blockquote>

<h4 id="object-recognition"><strong>Object recognition</strong></h4>

<p>The first step is being able to detect the images.</p>

<table>
<thead>
<tr>
<th align="center"><img src="/post/resources/img/1*VZPf2wq8AIjASwy0IfARCA.png" alt="" /></th>
</tr>
</thead>

<tbody>
<tr>
<td align="center">He loves those stools</td>
</tr>
</tbody>
</table>

<p>I wrote at length in my <a href="http://example.org/post/2018-04-05_how-to-actually-easily-detect-objects-with-deep-learning-on-raspberry-pi/">last post</a> about how object detection works, so I won’t go over much here. The file detect.py has a function called detect_from_image that creates the model based on provided config file and weights. The model takes about a minute to load from the config and weights (raspberry pi v3), but it only needs to load once.</p>

<p>The function then runs the image through the model and parses out the response, maps the object output to their respective class names and re-scales the response to the original image size and returns a dictionary of the objects detected.</p>

<p>Below is the signature of the main function in detect:</p>

<pre><code class="language-python">def detect_from_image(  
  image,  
  anchors_path,  
  classes_path,  
  config_path,  
  weights_path,  
  min_confidence,  
  max_overlap,  
  model_image_size):
</code></pre>

<p>The <strong>image</strong> can either be a string with the image path or a PIL.Image object.</p>

<p>The <strong>anchors_path</strong> is a list to the location of the anchors. The anchors are space separated values in a text file. The anchors for yolov2 are as follows:</p>

<p>0.57273, 0.677385, 1.87446, 2.06253, 3.33843, 5.47434, 7.88282, 3.52778, 9.77052, 9.16828</p>

<p>You can read more about anchors in my <a href="http://example.org/post/2018-04-05_how-to-actually-easily-detect-objects-with-deep-learning-on-raspberry-pi/">last post</a> under “Clustering Box Dimensions with Anchors”.</p>

<p>The <strong>classes_path</strong> just lists the classes that the model was trained on in a space separated text file.</p>

<p>The <strong>config_path</strong> holds the config file of the model. I tried saving the compiled model in .h5 format, but it caused a memory crash on my raspberry pi (v3). So I had to recreate the model every time through the config file and weights. I borrowed code from the great package <a href="https://github.com/allanzelener/YAD2K">yad2k</a> (Yet Another Darknet 2 Keras), but judging by the name, there are other options to convert the model to Keras.</p>

<p>The <strong>weights_path</strong>  is a path to the weights. The weights are too large to host on github, but they can be found <a href="https://pjreddie.com/darknet/yolov2/">here</a>.</p>

<p><strong>min_confidence</strong> is the minimum confidence level required to have the model detect and save down the location of an object. I found 50% works well</p>

<p><strong>max_overlap</strong> is the maximum amount of overlap allowed by the same object. The way that YOLO works means that the same object is likely to be detected several times with overlapping rectangles. In this case, you have to decide if the same object overlaps X% (intersection over union), then just take the object with a higher confidence level. If you set this too low, then you may not detect multiple objects of the same type if they are near each other. If set too high, then you may detect the same object multiple times. I found 0.35 to work well for my purposes.</p>

<p><img src="/post/resources/img/1*2LPQLE87SJBRCSXhpow9sA.png" alt="" /></p>

<p><strong>model_image_size</strong> is the size to rescale your image before running it through the network. I found (608,608) to work well.</p>

<p>The final output of the function will be similar to below and scaled to the original image size.</p>

<pre><code class="language-python">{'cat': [{'center': (1596.50, 1024.16),  
   'coors': [(2058.47, 1440.59),  
    (2982.40, 2273.46)],  
   'h': 832.86,  
   'w': 923.93,  
   'x': 2058.47,  
   'y': 1440.59}],  
 'chair': [{'center': (1569.28, 812.05),  
   'coors': [(2088.55, 1339.94),  
    (3127.08, 2395.71)],  
   'h': 1055.77,  
   'w': 1038.53,  
   'x': 2088.55,  
   'y': 1339.94}],  
 'suitcase': [{'center': (1330.33, 1028.88),  
   'coors': [(1606.64, 1172.80),  
    (2159.25, 1460.63)],  
   'h': 287.83,  
   'w': 552.61,  
   'x': 1606.64,  
   'y': 1172.80}]}
</code></pre>

<h4 id="tracking-the-objects">Tracking the Objects</h4>

<p>The tracking takes place in track.py. The file is fairly simple. Tracking can be accessed as follows:</p>

<pre><code class="language-sh">track.py [-h] [-i INTERVAL] [-s START] [-e END] [-d DIR] [--vflip] [--hflip]
</code></pre>

<p><strong>interval</strong> is the interval in minutes in which you want a picture to be taken</p>

<p><strong>start</strong> is the time you want the camera to start (e.g. “10 am”)</p>

<p><strong>end</strong> is the time you want the camera to end (e.g. “9 pm”)</p>

<p><strong>directory</strong> is the directory that you want the json logs to be saved</p>

<p><strong>vflip|hflip</strong> is a flag that tells the camera to vertically|horizontally flip the pictures taken</p>

<p>The file is short enough that its worth looking at in its entirety:</p>

<script type="application/javascript" src="//gist.github.com/breeko/9fa59558524c25b0012ef4768b3849d6.js"></script>

<h4 id="converting-logs-to-heat-maps-overlaid-on-images">Converting Logs to Heat-maps Overlaid on Images</h4>

<p>The final step is converting the logs into heat-maps</p>

<pre><code class="language-sh">convert.py [-h] [-d DIR] [-o OUTPUT] [-i INPUT] [-a ALPHA] [-b OBJECT]
</code></pre>

<p><strong>dir</strong> is the directory of the log (json) files that contain the object locations</p>

<p><strong>output</strong> is the output of the final image</p>

<p><strong>input</strong> is the base image to be drawn over</p>

<p><strong>alpha</strong> is the redness of the alpha of the circles detecting the objects (higher is darker)</p>

<p><strong>object</strong> is the object that you want to detect. If you have a lot of objects detected over a long period of time, you would want to set the alpha to be low, and vice versa.</p>

<p>I didn’t run this long enough to capture any interesting cat location analytics but below is a the final output detecting the placement of cups in my living room throughout the day.</p>

<table>
<thead>
<tr>
<th align="center"><img src="/post/resources/img/1*pE3JF2dGes3h6aMntdcmzQ.png" alt="" /></th>
</tr>
</thead>

<tbody>
<tr>
<td align="center">Someone moved my cup!</td>
</tr>
</tbody>
</table>

<p>The code for the heatmap was taken from the <a href="https://stackoverflow.com/questions/42481203/heatmap-on-top-of-image">github answer</a> on a similar question. It is more of a hack and can definitely be improved. The underlying picture becomes faded and its sometimes tricky to get the right alpha balance but it serves its purpose here.</p>

<h3 id="next-steps">Next Steps</h3>

<h4 id="detect">Detect</h4>

<ol>
<li>Consider changing over to have Keras do all the manipulations, as opposed to numpy originally used for debugging</li>
<li>Add support for other machine learning models. I couldn’t get the yolo-mini to produce any meaningful detections so there may be a bug somewhere</li>
<li>Add support for raspberry pi zero</li>
</ol>

<h4 id="track">Track</h4>

<ol>
<li>Consider using detection to detect when objects in the frame are moving. This will free up a lot of time being spent running the same image through the model and allow for a higher frame count</li>
<li>Take another look at pickling the model file and reducing the time it takes to load</li>
<li>More flexibility on start and end input types [DONE]</li>
</ol>

<h4 id="convert">Convert</h4>

<ol>
<li>Re-think how the heatmap is drawn</li>
<li>Expand the convert functionality to support multiple objects being detected and drawn with varying colors and a key</li>
</ol>

<h4 id="yad2k-out">yad2k_out</h4>

<ol>
<li>Optional verbosity [DONE]</li>
</ol>

<h3 id="final-thoughts">Final Thoughts</h3>

<p>A lot of people in the machine learning space are trying to sell you an API and real-time object detection using freely available models developed by others. The fact is that for many applications, you don’t need real-time detection and the models are small enough to run anywhere. It may take you 30 seconds to evaluate the image, but that’s fine for many purposes.</p>

<p>This functionality is very low cost, secure and respectful of privacy. The fact that its cheap to build, relatively simple, fully customizable and able to run offline puts it at an advantage over many more expensive cloud services.</p>

<p>Discussion can be found at <a href="https://news.ycombinator.com/item?id=17029430">hacker news</a>.</p>

<p>By Branko Blagojevic on May 9, 2018</p>

</div>

        <footer class="post-footer clearfix">
    

    <div class="share">
        
            <a class="icon-twitter" href="https://twitter.com/share?text=Offline%20Object%20Detection%20and%20Tracking%20on%20a%20Raspberry%20Pi&url=http%3a%2f%2fexample.org%2fpost%2f2018-05-09_offline-object-detection-and-tracking-on-a-raspberry-pi%2f"
                onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                <i class="fa fa-twitter"></i>
                <span class="hidden">Twitter</span>
            </a>
        

        
            <a class="icon-facebook" href="https://www.facebook.com/sharer/sharer.php?u=http%3a%2f%2fexample.org%2fpost%2f2018-05-09_offline-object-detection-and-tracking-on-a-raspberry-pi%2f"
                onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
                <i class="fa fa-facebook"></i>
                <span class="hidden">Facebook</span>
            </a>
        

        
        
    </div>
</footer>

        
    </article>
</div>

            </div>
        </div>

        <footer class="footer">
            <div class="container">
                <div class="site-title-wrapper">
                    <h1 class="site-title">
                        <a title="ML-Everything" href="http://example.org/">ML-Everything</a>
                    </h1>
                    <a class="button-square button-jump-top js-jump-top" href="#">
                        <i class="fa fa-angle-up"></i>
                    </a>
                </div>

                <p class="footer-copyright">
                    <span>&copy; 2018 / Powered by <a href="https://gohugo.io/">Hugo</a></span>
                </p>
                <p class="footer-copyright">
                    <span><a href="https://github.com/roryg/ghostwriter">Ghostwriter theme</a> By <a href="http://jollygoodthemes.com">JollyGoodThemes</a></span>
                    <span>/ <a href="https://github.com/jbub/ghostwriter">Ported</a> to Hugo By <a href="https://github.com/jbub">jbub</a></span>
                </p>
            </div>
        </footer>

        <script src="http://example.org/js/jquery-1.11.3.min.js"></script>
        <script src="http://example.org/js/jquery.fitvids.js"></script>
        
        
            <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        
        
        <script src="http://example.org/js/scripts.js"></script>
    </body>
</html>

