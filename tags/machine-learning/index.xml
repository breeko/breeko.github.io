<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>machine-learning on ML-Everything</title>
    <link>https://breeko.github.io/tags/machine-learning/</link>
    <description>Recent content in machine-learning on ML-Everything</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 03 Sep 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://breeko.github.io/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Demo Apps and Building a Business off GPT-3</title>
      <link>https://breeko.github.io/post/2020-09-03-demo-apps-and-building-a-business-off-gpt3/</link>
      <pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://breeko.github.io/post/2020-09-03-demo-apps-and-building-a-business-off-gpt3/</guid>
      <description>Demo Apps and Building a Business off GPT-3 Here’s a cool app. Give a description of a website and have one built for you using GPT-3 magic. Here’s an example:
A website like apple.com for twitter  But with demos like this, you have to read below the fold:
Copy is being generous, since its only one line. So GPT-3 is producing the essentially: “Twitter. We believe that everyone deserves the right to express themselves” and maybe the tab text.</description>
    </item>
    
    <item>
      <title>Using GPT-3 to Explain Jokes</title>
      <link>https://breeko.github.io/post/2020-09-02-using-gpt3-to-explain-jokes/</link>
      <pubDate>Wed, 02 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://breeko.github.io/post/2020-09-02-using-gpt3-to-explain-jokes/</guid>
      <description>Using GPT-3 to Explain Jokes There’s a Reddit bot that explains jokes. Well, that’s not exactly true, because its too good to be a bot and is curiously selective. Often times, the explanation is funnier than the joke. It hasn’t been active in about a year, but here’s an example:
 My wife and I went on our honeymoon to Australia. Unfortunately, I had to dial the help line.
  &amp;ldquo;G&amp;rsquo;day this is Tim, you&amp;rsquo;ve reached the Aussie help line.</description>
    </item>
    
    <item>
      <title>How Facial Recognition Works Part 4, Comparing Congressmen to Mugshots</title>
      <link>https://breeko.github.io/post/2019-09-30-how-facial-recognition-works-part-4/</link>
      <pubDate>Thu, 26 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://breeko.github.io/post/2019-09-30-how-facial-recognition-works-part-4/</guid>
      <description>This is part 4 of my series on facial recognition. Part 1 was about face detection, part 2 was about facial landmarks and part 3 was about accuracy over time.
This post I will compare US congressmen to a public dataset of mugshots. This was inspired by the ACLU claim that Amazon’s face recognition falsely matched 28 members of Congress with mugshots.
The ACLU post didn&amp;rsquo;t include any source code or source of the mugshot datasets or the congressman dataset.</description>
    </item>
    
    <item>
      <title>How Facial Recognition Works Part 3, How a Face Changes Over 10 Years</title>
      <link>https://breeko.github.io/post/2019-09-23-how-facial-recognition-works-part-3/</link>
      <pubDate>Mon, 23 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://breeko.github.io/post/2019-09-23-how-facial-recognition-works-part-3/</guid>
      <description>This is part 3 of a series of posts about facial recognition. In part 1 I discussed facial detection and part 2 I discussed facial landmarks. In this post, I&amp;rsquo;ll go into how accurate some off the shelf facial recognition are and how well they work as a subject ages.
I found this video of a man (boy?) that took a picture of himself every day over 10 years starting at age 12.</description>
    </item>
    
    <item>
      <title>How Facial Recognition Works Part 2, Facial Landmarks</title>
      <link>https://breeko.github.io/post/2019-07-16-how-facial-recognition-works-part-2/</link>
      <pubDate>Tue, 17 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://breeko.github.io/post/2019-07-16-how-facial-recognition-works-part-2/</guid>
      <description>In my last post I discussed how face detection works. In this post I&amp;rsquo;ll discuss how landmark detection works.
Python offers some great free libraries for face recognition. Sorry, thats a lie. They offer great wrappers for C++ libraries like dlib. Using face_recognition, we can identify the face locations of an image is as simple as face_recognition.face_locations.
When talking about what distinguishes one face from another we normally talk about facial landmarks.</description>
    </item>
    
    <item>
      <title>How Facial Recognition Works Part 1: Face Detection</title>
      <link>https://breeko.github.io/post/2019-07-08-how-facial-recognition-works/</link>
      <pubDate>Mon, 08 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://breeko.github.io/post/2019-07-08-how-facial-recognition-works/</guid>
      <description>Much has been written about facial recognition and its role in society. As with a lot of topics concerning artificial intelligence, it is often discussed with an air of mysticism, as though all the all seeing machines can now peer into our souls. Depending on the narrative of the piece, its accuracy is chillingly accurate or appallingly inadequate for use. Amazon got flack for pitching their Reckognition service to law enforcement, San Fransisco banned the technology, and it has even been called racist.</description>
    </item>
    
    <item>
      <title>Building an Object Detection API with AWS S3, Rekognition and Lambda</title>
      <link>https://breeko.github.io/post/2019-03-18-building-an-object-detection-api-with-aws-s3-rekognition-and-lambda/</link>
      <pubDate>Mon, 18 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://breeko.github.io/post/2019-03-18-building-an-object-detection-api-with-aws-s3-rekognition-and-lambda/</guid>
      <description>In a prior post, Offline Object Detection and Tracking on a Raspberry Pi, I wrote an implementation of an offline object detection model. Since that time, aws has begun to offer some great services in that space, so I felt it was time to write about building an online object detection model with aws.
Use-Case My use-case is the same as the original article: take a picture periodically, process it, and display some information as to the location of given objects over time.</description>
    </item>
    
    <item>
      <title>Generating Letters Using Generative Adversarial Networks (GANs)</title>
      <link>https://breeko.github.io/post/2018-11-05-generating-letters-using-generative-adversarial-networks-gans/</link>
      <pubDate>Mon, 05 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://breeko.github.io/post/2018-11-05-generating-letters-using-generative-adversarial-networks-gans/</guid>
      <description>A generative adversarial network is a set of competing models that simultaneously learn to generate and discriminate real images from fake
In my last post post I wrote about variational auto encoders (VAEs). A VAE is composed of an encoder and decoder and can be used to compress and generate data.
In this post I’ll discuss Generative Adversarial Networks (GANs). Like VAEs, GANs are made up of two models: a generator and a discriminator and can also be used to generate data.</description>
    </item>
    
    <item>
      <title>Another &#39;Variational Auto Encoders Explained&#39; Post and Character Encoding</title>
      <link>https://breeko.github.io/post/2018-10-25_another-variational-auto-encoders-explained-post-and-character-encoding/</link>
      <pubDate>Thu, 25 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://breeko.github.io/post/2018-10-25_another-variational-auto-encoders-explained-post-and-character-encoding/</guid>
      <description>Variational Auto Encoders are simpler than they appear and are the building blocks of generative models
Variational Auto Encoders are used to learn probability distribution function of images. By viewing their latent space we can see what’s really going on and use it to generate our own examples
I have read a fair number of posts about variational auto encoders but only recently did I try my hand in generating my own.</description>
    </item>
    
    <item>
      <title>AI, Optimists vs Pessimists and Why The Singularity Isn’t Near</title>
      <link>https://breeko.github.io/post/2018-10-08_ai-optimists-vs-pessimists-and-why-the-singularity-isn-t-near/</link>
      <pubDate>Mon, 08 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://breeko.github.io/post/2018-10-08_ai-optimists-vs-pessimists-and-why-the-singularity-isn-t-near/</guid>
      <description>AI has its pessimists and optimists. The one thing they agree on is that the singularity is just around the corner. It’s not.
AI optimists and pessimists both agree that the singularity is just around the corner. Both think it will transform society. But practitioners think not much of anything will happen.
Last year I read Ray Kurzweil’s book How to Create a Mind and it made quite an impression on me.</description>
    </item>
    
    <item>
      <title>Offline Object Detection and Tracking on a Raspberry Pi</title>
      <link>https://breeko.github.io/post/2018-05-09_offline-object-detection-and-tracking-on-a-raspberry-pi/</link>
      <pubDate>Wed, 09 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://breeko.github.io/post/2018-05-09_offline-object-detection-and-tracking-on-a-raspberry-pi/</guid>
      <description>Load and run YOLO (You Only Look Once) object detection model on a Raspberry Pi and track objects throughout the day.
In my last post I wrote about the YOLO model used for object detection. The most surprising thing was how simple the model is. It’s so simple that it can run offline on a raspberry pi
In my last post I wrote about the YOLO (You Only Look Once) model used for object detection.</description>
    </item>
    
    <item>
      <title>How to (actually) easily detect objects with deep learning on raspberry pi</title>
      <link>https://breeko.github.io/post/2018-04-05_how-to-actually-easily-detect-objects-with-deep-learning-on-raspberry-pi/</link>
      <pubDate>Thu, 05 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://breeko.github.io/post/2018-04-05_how-to-actually-easily-detect-objects-with-deep-learning-on-raspberry-pi/</guid>
      <description>YOLO provides state of the art real-time object detection and classification.
The YOLO image detection model is one of the fastest and most accurate object detection models. Flexible and fast, YOLO is a huge step forward in machine learning.
I came across a popular post on hackernews titled How to easily Detect Objects with Deep Learning on Raspberry Pi. The article discusses the YOLO object detection model that can be used for real-time object detection and classification.</description>
    </item>
    
    <item>
      <title>Reinforcement learning with sparse rewards</title>
      <link>https://breeko.github.io/post/2018-02-26_reinforcement-learning-with-sparse-rewards/</link>
      <pubDate>Mon, 26 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://breeko.github.io/post/2018-02-26_reinforcement-learning-with-sparse-rewards/</guid>
      <description>This is a continuation of a series of posts on reinforcement learning. This will continue the discussion from the last post and use the…
This is a continuation of a series of posts on reinforcement learning. This will continue the discussion from the last post) and use the OpenAI environment discussed in an earlier post.
Reinforcement learning shouldn’t be hard. The idea is simple enough:
 Try some things randomly and save down the states and the rewards Train a network to predict the reward Use the network to choose the highest reward, allowing for some randomness Continue to train based on those experiences  You’ll read tutorials that get everything to work swimmingly.</description>
    </item>
    
    <item>
      <title>Learning from pixels and Deep Q-Networks with Keras</title>
      <link>https://breeko.github.io/post/2018-02-20_learning-from-pixels-and-deep-q-networks-with-keras/</link>
      <pubDate>Tue, 20 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://breeko.github.io/post/2018-02-20_learning-from-pixels-and-deep-q-networks-with-keras/</guid>
      <description>This is a continuation of my series on reinforcement learning. I’d recommend the second post if you’re unfamiliar with OpenAI and the third…
This is a continuation of my series on reinforcement learning. I’d recommend the second post if you’re unfamiliar with OpenAI and the third post if you’re unfamiliar with using neural networks to train a policy. This borrows some code and ideas from Arthur Juliani’s post, especially the game environment.</description>
    </item>
    
    <item>
      <title>Policy Based Reinforcement Learning with Keras</title>
      <link>https://breeko.github.io/post/2018-02-12_policy-based-reinforcement-learning-with-keras/</link>
      <pubDate>Mon, 12 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://breeko.github.io/post/2018-02-12_policy-based-reinforcement-learning-with-keras/</guid>
      <description>This post will discuss reinforcement learning through policy based agents on an OpenAI environment
Policy based reinforcement learning is simply training a neural network to remember the actions that worked best in the past. This framework provides incredible flexibility and works across many envs
This post will discuss reinforcement learning through policy based agents. We’ll be using OpenAI’s gym environment which I discussed on my last post. The next few posts are heavily influenced by Arthur Juliani’s great series on reinforcement learning.</description>
    </item>
    
    <item>
      <title>Evolutionary Learning Models with OpenAI</title>
      <link>https://breeko.github.io/post/2018-02-05_evolutionary-learning-models-with-openai/</link>
      <pubDate>Mon, 05 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://breeko.github.io/post/2018-02-05_evolutionary-learning-models-with-openai/</guid>
      <description>Evolutionary learning models are a great introduction to machine learning because they’re simple to understand conceptually and…
Evolutionary learning models are a great introduction to machine learning because they’re simple to understand conceptually and surprisingly easy to implement. In this post, I’ll create an evolutionary learning model to use on the OpenAI. The great thing about OpenAI is the simple API and large number of environments to experiment on. So any bot we create in one environment can (in theory) be used on any other environment.</description>
    </item>
    
    <item>
      <title>Tic-Tac-Toe and Connect-4 using Mini-Max</title>
      <link>https://breeko.github.io/post/2018-01-29_tic-tac-toe-and-connect-4-using-mini-max/</link>
      <pubDate>Mon, 29 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://breeko.github.io/post/2018-01-29_tic-tac-toe-and-connect-4-using-mini-max/</guid>
      <description>A lot of ink has been spilled about machine learning techniques being used to beat people in games. From Deep Blue (chess) in the 1990s, to…
A lot of ink has been spilled about machine learning techniques being used to beat people in games. From Deep Blue (chess) in the 1990s, to AlphaGo (Go) more recently, computers seem to be performing well against their human counterparts. Even games like poker aren’t completely safe from the machine takeover.</description>
    </item>
    
  </channel>
</rss>